<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Flappy Bird PPO Agent | Geoff Goodwin-Wilson</title>
  <link rel="stylesheet" href="../styles.css" />
  <script src="/sidebar.js?v=2" defer></script>
</head>
<body>
  <div class="site">
    <aside class="site-sidebar" id="site-sidebar"></aside>

    <main class="site-main">
      <section class="page-intro">
        <a class="back-link" href="../index.html">← Back to projects</a>
        <p class="eyebrow">Project</p>
        <h1 class="page-title">Flappy Bird PPO Agent</h1>
        <div class="project-meta">November 2025 · Reinforcement Learning</div>
      </section>

      <article class="entry-content flappy-bird">
        <h2>Introduction</h2>
        <p>The goal of this project is to train an agent to solve Flappy Bird (gymnasium FlappyBird-v0) using LIDAR observations. My target is to first surpass my personal high score of 120 and ultimately attempt to beat the world record of 2607.</p>
        <p>While the majority of introductory Reinforcement Learning (RL) projects rely on simple Multi-Layer Perceptrons (MLPs) or Convolutional Neural Networks (CNNs), the recent success of Large Language Models has sparked intense interest in applying Transformer architectures to other applications and modalities. This post analyzes the implementation of a standard MLP-based agent versus a Transformer-based agent, exploring how self-attention mechanisms can act as a temporal feature extractor for state-based control.</p>

        <figure class="entry-figure high-score2">
          <img src="../assets/flappy-bird-images/high-score2.gif" alt="High score gameplay" />
          <figcaption class="figure-subtext">Flappy Bird gameplay using an autonomous agent</figcaption>
        </figure>

        <p>The agent is trained via RL and built from scratch using PyTorch. I implemented Proximal Policy Optimization (PPO), featuring a modular architecture that supports both MLP and Transformer-Encoder backbones. PPO, introduced by OpenAI in 2017, is famously known for solving the OpenAI Rubik’s Cube robotics challenge and defeating Dota 2 professionals.</p>
        <p>I chose PPO for this environment for several reasons:</p>
        <ul>
          <li>Stability: Policy improvement clipping prevents drastic, destructive updates.</li>
          <li>Robustness: It is generally less sensitive to hyperparameter tuning than off-policy methods.</li>
          <li>Sample Efficiency: For an on-policy algorithm, it reuses training data effectively across multiple epochs.</li>
        </ul>
        <p>There are downsides, though they are less relevant for this specific task. As an on-policy method, the logistics of training can be challenging, and it is sometimes outperformed by other algorithms on continuous output tasks. However, since Flappy Bird is a lightweight environment with discrete outputs (flap or don't flap), PPO is a good choice.</p>

        <h2>The Naïve Model Approach: MLP</h2>
        <p>I initially hypothesized that this environment could be solved using a simple MLP for both the agent model and value function. If we can solve it with a simple architecture, there is no need to overcomplicate things; if not, it serves as a solid baseline. To test this theory, I implemented 128 and 512-dimension, 2-layer networks.</p>

        <figure class="entry-figure mlp">
          <img src="../assets/flappy-bird-images/mlp-diagram.webp" alt="Simple MLP model architecture" />
          <figcaption class="figure-subtext">Simple MLP model architecture</figcaption>
        </figure>

        <h2>The Training Process</h2>
        <p>The core of this system is the Agent class and the trajectory rollout. The algorithm works as follows:</p>
        <ol>
          <li>Play the game using the current policy and collect a fixed length of samples (a “trajectory”).</li>
          <li>Save all PPO-relevant variables in memory: observation, action, action probability, value, reward, and done flags.</li>
          <li>Calculate the loss objectives and backpropagate them through the policy and value models.</li>
        </ol>
        <p>Using PPO to train the model turned out to be straightforward. The agent improved quickly, achieving scores up to 100 within an hour. However, training eventually plateaued, taking a full day to achieve a score of 1000. Note that I trained this on a laptop RTX 4060 GPU—significantly slower than state-of-the-art GPU hardware.</p>
        <p>As the agent gains experience, we start to see diminishing returns per training cycle. The error signals become sparse once the agent reaches higher scores; essentially, if the agent rarely fails, it rarely learns from its mistakes.</p>

        <h2>MLP Results &amp; The "Partial Observability" Problem</h2>
        <p>The MLP achieved surprisingly good results, but its performance seems to be bound by the observation limits. In specific scenarios, the agent fails because it cannot properly observe its surroundings.</p>
        <p>If the bird is close to the corner of a pipe as it passes, the LIDAR signals may not "see" the bottom corner, leading to a collision. Consider the following case: the bird’s "vision" has passed the pipe, but the bird's hitbox has not. This results in failure with no apparent correlation in the observation space regarding what went wrong.</p>

        <figure class="entry-figure bird-collision">
          <img src="../assets/flappy-bird-images/bird-collision.webp" alt="Hitbox edge colliding with pipe" />
          <figcaption class="figure-subtext">Hitbox edge colliding with pipe, despite no obstacles in observation space.</figcaption>
        </figure>

        <p>The fundamental problem is that the environment is only a Partially Observable Markov Decision Process in this state. We cannot see the true state, only incomplete observations. Because the bird can only see in front of it, not around it, the Markov property, which assumes the current state contains all necessary information to decide an action, is broken.</p>
        <p>To fix this, we need to add previous states to our observation. This history should allow the agent to remember the pipe location and act before a collision occurs.</p>
        <p>We have a few options to inject past states into the model:</p>
        <ol>
          <li>Expand the MLP Input: Concatenate 5-10 past frames. With a fully connected net, this drastically increases the parameter count.</li>
          <li>Recurrent Networks (RNN/LSTM): Use hidden state vectors to implicitly store temporal information.</li>
          <li>Transformers: Implement a Transformer with past observations as input "tokens" and use self-attention to relate them.</li>
        </ol>
        <p>For an agent this simple, an RNN or expanded MLP is likely the optimal engineering solution due to inference speed. Despite this, I chose the "overkill" Transformer solution to explore the topology in a control context.</p>

        <h2>Implementing the Transformer</h2>
        <h3>1. Input Embedding &amp; Positional Encoding</h3>
        <p>The environment returns a LiDAR-based state vector (<span class="math">d<sub>in</sub>=180</span>). To process this via a Transformer, I first project this feature vector into a higher-dimensional embedding space (<span class="math">d<sub>model</sub>=512</span>) using a learnable linear layer followed by LayerNorm.</p>
        <p>Because the Transformer is permutation-invariant (it sees the sequence as a "bag of states" rather than a timeline), we must inject temporal order. I implemented Absolute Sinusoidal Positional Encodings:</p>
        <div class="math-block"><span class="math">PE<sub>(pos, 2i)</sub> = sin(pos / 10000<sup>2i/d<sub>model</sub></sup>)</span></div>
        <div class="math-block"><span class="math">PE<sub>(pos, 2i+1)</sub> = cos(pos / 10000<sup>2i/d<sub>model</sub></sup>)</span></div>
        <p>This allows the model to generalize to sequence lengths potentially unseen during training and provides a distinct geometric relationship between timestamps <span class="math">t</span> and <span class="math">t-k</span>.</p>

        <h3>2. The Attention Mechanism</h3>
        <p>The heart of the policy is the Multi-Head Attention block. This mechanism allows different temporal observations to influence one another. The model computes Query (<span class="math">Q</span>), Key (<span class="math">K</span>), and Value (<span class="math">V</span>) matrices from the input sequence, and attention weights are calculated via a scaled dot-product:</p>
        <div class="math-block"><span class="math">Attention(Q, K, V) = softmax((QK<sup>T</sup>)/sqrt(d<sub>k</sub>)) · V</span></div>
        <p>In the context of Flappy Bird, this allows the agent to "attend" to specific past states that are most relevant to the current decision. For example, the observation of the bird 3 frames ago might be more critical for determining a jump action than the observation 10 frames ago.</p>

        <figure class="entry-figure">
          <img src="../assets/flappy-bird-images/part-trained-attention.gif" alt="Partially trained attention weights" />
          <figcaption class="figure-subtext">Animation of the attention weights for each head and layer during gameplay.</figcaption>
        </figure>

        <p>Interpreting these results is a challenge, but it appears the Transformer is learning distinct features of the temporal data. I noticed that as model training progressed, the attention map became much sharper and more certain about which timesteps mattered.</p>

        <figure class="entry-figure">
          <img src="../assets/flappy-bird-images/fully-trained-attention.gif" alt="Fully trained transformer attention visualization" />
          <figcaption class="figure-subtext">Fully trained transformer attention visualization</figcaption>
        </figure>

        <h3>3. Value Head vs. Policy Head</h3>
        <p>The Transformer backbone is shared between the policy and value heads. Since they both rely on the same state history to derive their output and they’re trained simultaneously, sharing the backbone is computationally efficient. The final token's embedding (<span class="math">x[:, -1, :]</span>) is passed to two separate linear heads:</p>
        <ul>
          <li>Policy Head: Outputs logits for 2 actions (Flap / Do Nothing).</li>
          <li>Value Head: Outputs a scalar estimate of the state value.</li>
        </ul>

        <h2>Training Stability: Using Entropy to Encourage Exploration</h2>
        <p>I encountered a significant issue during training where the model got stuck in a permanent "flap" action loop, causing the agent to hit the ceiling and lose immediately.</p>
        <p>My hypothesis is that because the sparse reward signal is difficult to find, the agent discovers a local minimum (flapping constantly) and decides it is the "safest" policy.</p>

        <figure class="entry-figure">
          <img src="../assets/flappy-bird-images/failure-path.gif" alt="Agent stuck in flap loop" />
          <figcaption class="figure-subtext">Agent always takes the same "flap" path, ending in premature failure.</figcaption>
        </figure>

        <p>To rectify this, I utilized the entropy bonus (ent_coef=0.02) in the loss function, a standard PPO feature. This penalizes the model for being too confident too early, effectively preventing the probability distribution from collapsing into a deterministic policy. This forces the agent to keep its options open, widening the experience pool by rewarding the exploration of alternative actions.</p>

        <figure class="entry-figure">
          <img src="../assets/flappy-bird-images/entropy-ablation.webp" alt="Training progress with and without entropy bonus" />
          <figcaption class="figure-subtext">A comparison of training progress with and without the entropy bonus, using a simple MLP.</figcaption>
        </figure>

        <h2>Conclusion &amp; Future Work</h2>
        <p>This project demonstrates that Transformers can successfully solve low-dimensional continuous control tasks when treated as sequence modeling problems. By replacing the Markov assumption (current state is all that matters) with a sequence history processed via self-attention, the agent learns robust policies that account for previous observations even when the current sensor data is incomplete.</p>
        <p>Future iterations of this project could explore the use of RNNs or expanded-input MLPs to compare the inference/training efficiency against the Transformer approach. It’s also worth exploring vectorized environments to unlock large speedups in training. I did not look into GPU optimization for this project, and it’s possible that much higher performance is possible with the same hardware setup.</p>

        <h2>Appendix</h2>

        <h3>Training Algorithm Breakdown</h3>
        <p>The agent is trained using Proximal Policy Optimization (PPO), an on-policy gradient method that strikes a balance between ease of implementation and sample efficiency.</p>

        <h3>Generalized Advantage Estimation (GAE)</h3>
        <p>To reduce the variance of the gradient estimates, GAE is used. Instead of using raw rewards in PPO, we calculate the Advantage <span class="math">A<sub>t</sub></span>, which measures how much better an action was compared to the baseline value function <span class="math">V(s)</span>. The code computes the Temporal Difference (TD) error <span class="math">&delta;<sub>t</sub></span> first:</p>
        <div class="math-block"><span class="math">&delta;<sub>t</sub> = r<sub>t</sub> + &gamma; V(s<sub>t+1</sub>) - V(s<sub>t</sub>)</span></div>
        <p>Then, it computes the exponentially weighted average of these errors:</p>
        <div class="math-block"><span class="math">A<sub>t</sub> = &Sigma;<sub>k=0</sub><sup>T-t-1</sup> (&gamma;&lambda;)<sup>k</sup> &delta;<sub>t+k</sub></span></div>

        <h3>The Clipped Surrogate Objective</h3>
        <p>To prevent catastrophic forgetting (where a large gradient update destroys the policy), PPO constrains the policy update. We calculate the probability ratio between the new policy and the old policy making the same action:</p>
        <div class="math-block"><span class="math">r<sub>t</sub>(&theta;) = &pi;<sub>&theta;</sub>(a<sub>t</sub>|s<sub>t</sub>)/&pi;<sub>&theta;<sub>old</sub></sub>(a<sub>t</sub>|s<sub>t</sub>)</span></div>
        <p>If this probability changes too drastically, we clip the update. The loss function is defined as:</p>
        <div class="math-block"><span class="math">L<sup>CLIP</sup>(&theta;) = E<sub>t</sub> [ min(r<sub>t</sub>(&theta;)A<sub>t</sub>, clip(r<sub>t</sub>(&theta;), 1-&epsilon;, 1+&epsilon;)A<sub>t</sub>) ]</span></div>

        <h3>Gradient Clipping</h3>
        <p>To prevent exploding gradients—common in Transformers due to their depth—torch.nn.utils.clip_grad_norm_ is applied before the optimizer step, capped at 0.5.</p>

        <h3>Supplemental Model Specifications</h3>
        <p><strong>MLP Based Model:</strong></p>
        <ul>
          <li>Policy &amp; Value Network: Separate</li>
          <li>Input Dimension: 180 (observation dim)</li>
          <li>Hidden Dimension: 128 / 512</li>
          <li>Output Dimension: 2 (probabilities)</li>
          <li>Layers: 2 (Fully Connected)</li>
          <li>Number of Parameters: [Insert Count]</li>
        </ul>

        <p><strong>Transformer Based Model:</strong></p>
        <ul>
          <li>Policy &amp; Value Network: Combined (Shared Backbone)</li>
          <li>Embedding Dimension (<span class="math">d<sub>model</sub></span>): 128 / 512</li>
          <li>Attention Heads: 4</li>
          <li>Layers: 4</li>
          <li>Sequence Length: 10 (Sliding window)</li>
          <li>Feed Forward Dim: 512 / 2048 (<span class="math">4 &times; d<sub>model</sub></span>)</li>
          <li>Activation: GELU</li>
          <li>Normalization: LayerNorm (Pre-norm)</li>
          <li>Dropout: 0.1</li>
          <li>Number of Parameters: [Insert Count]</li>
        </ul>

        <p><strong>PPO Hyperparameters</strong></p>
        <ul>
          <li>Optimizer: Adam (lr=3e-4)</li>
          <li>Gamma (<span class="math">&gamma;</span>): 0.99 (Discount factor)</li>
          <li>Lambda (<span class="math">&lambda;</span>): 0.95 (GAE smoothing)</li>
          <li>Clip Range (<span class="math">&epsilon;</span>): 0.2</li>
          <li>Rollout Buffer: 256 steps</li>
          <li>Minibatch Size: 32</li>
          <li>Epochs per Rollout: 3</li>
        </ul>
      </article>
    </main>
  </div>
</body>
</html>
