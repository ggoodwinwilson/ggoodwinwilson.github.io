<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Vision-Language Action Model Analysis | Geoff Goodwin-Wilson</title>
  <link rel="stylesheet" href="../styles.css" />
</head>
<body>
  <div class="site">
    <aside class="site-sidebar">
      <div class="avatar" aria-hidden="true">GG</div>
      <div class="brand-name">Geoff Goodwin-Wilson</div>
      <div class="brand-role">Electrical Engineering / Machine Learning</div>
      <p class="sidebar-bio">Quiet logbook for robotics and ML systems built with an electrical engineering toolkit.</p>

      <div class="sidebar-section">
        <div class="sidebar-heading">Downloads</div>
        <div class="sidebar-links">
          <a class="sidebar-link" href="../assets/GeoffGoodwinWilson_Resume.pdf" download>Resume (PDF)</a>
          <a class="sidebar-link" href="../assets/GeoffGoodwinWilson_Portfolio.pdf" download>Portfolio (PDF)</a>
        </div>
      </div>

      <div class="sidebar-section">
        <div class="sidebar-heading">Elsewhere</div>
        <div class="sidebar-links">
          <a class="sidebar-link" href="https://twitter.com/geoffg_w" target="_blank" rel="noreferrer">
            <svg viewBox="0 0 24 24" aria-hidden="true"><path d="M4 4l7.5 8.3L4.5 20H7l5.3-6.3L16.5 20H20l-7.6-8.4L19.5 4H17l-4.7 5.6L8 4H4z" fill="currentColor" /></svg>
            X / Twitter
          </a>
          <a class="sidebar-link" href="https://github.com/geoffgoodwinwilson" target="_blank" rel="noreferrer">
            <svg viewBox="0 0 24 24" aria-hidden="true"><path d="M12 2C6.48 2 2 6.58 2 12.17c0 4.48 2.87 8.28 6.84 9.63.5.1.68-.22.68-.48 0-.24-.01-.87-.01-1.7-2.78.62-3.37-1.36-3.37-1.36-.45-1.17-1.1-1.48-1.1-1.48-.9-.63.07-.62.07-.62 1 .07 1.53 1.05 1.53 1.05.89 1.56 2.34 1.11 2.91.85.09-.66.35-1.1.64-1.35-2.22-.26-4.56-1.13-4.56-5 0-1.1.38-2 .99-2.7-.1-.26-.43-1.29.09-2.69 0 0 .83-.27 2.72 1.02a9.2 9.2 0 0 1 4.95 0c1.89-1.29 2.72-1.02 2.72-1.02.52 1.4.19 2.43.09 2.69.62.7 1 1.6 1 2.7 0 3.88-2.34 4.74-4.57 5 .36.32.69.95.69 1.92 0 1.39-.01 2.51-.01 2.85 0 .26.18.58.69.48A10.18 10.18 0 0 0 22 12.17C22 6.58 17.52 2 12 2z" fill="currentColor" /></svg>
            GitHub
          </a>
          <a class="sidebar-link" href="https://www.linkedin.com/in/geoff-goodwin-wilson" target="_blank" rel="noreferrer">
            <svg viewBox="0 0 24 24" aria-hidden="true"><path d="M6.94 21H3.43V8.9h3.51V21zM5.18 7.3c-1.13 0-2.06-.95-2.06-2.12C3.12 4 4.05 3 5.18 3s2.06 1 2.06 2.18c0 1.17-.93 2.12-2.06 2.12zM20.89 21h-3.5v-5.87c0-1.4-.03-3.2-1.94-3.2-1.94 0-2.24 1.5-2.24 3.1V21H9.7V8.9h3.36v1.65h.05c.47-.9 1.63-1.85 3.36-1.85 3.6 0 4.27 2.42 4.27 5.56V21z" fill="currentColor" /></svg>
            LinkedIn
          </a>
        </div>
      </div>
    </aside>

    <main class="site-main">
      <section class="page-intro">
        <a class="back-link" href="../index.html">← Back to projects</a>
        <p class="eyebrow">Project</p>
        <h1 class="page-title">Vision-Language Action Model Analysis</h1>
        <div class="project-meta">2024 · Robotics ML</div>
        <p class="lede">Evaluation of a vision-language action model for tabletop manipulation: how prompt wording, camera placement, and grounding strategies change action quality.</p>
        <div class="meta-grid">
          <div class="meta-box"><strong>Focus</strong><span>Grounding, evaluation harness</span></div>
          <div class="meta-box"><strong>Stack</strong><span>Python, PyTorch, OpenCV</span></div>
          <div class="meta-box"><strong>Artifacts</strong><span>Prompts, logs, overlay videos</span></div>
        </div>
      </section>

      <article class="entry-content">
        <h2>Overview</h2>
        <p>Built a repeatable test rig for vision-language policies: synchronized RGB feed, prompt variants, and logged action traces with spatial overlays. The goal was to see where natural language breaks down before any real hardware time.</p>

        <h2>Highlights</h2>
        <ul>
          <li>Measured grounding accuracy versus latency across different camera angles and prompt phrasings.</li>
          <li>Overlayed predicted actions on frames to expose failure modes like swapped coordinates and drift.</li>
          <li>Packaged prompts, evaluation scripts, and sample traces for later reuse in control stacks.</li>
        </ul>

        <div class="media-placeholder">Drop comparison diagrams or action trace GIFs here.</div>

        <h2>Findings</h2>
        <p>Small prompt rewrites changed grasp intent more than expected; camera height mattered less than table-relative language. Latency spikes correlated with longer prompt templates, not model size.</p>

        <h2>Artifacts</h2>
        <p>Prompt set, action overlays, and per-seed logs live with the project; export to collaborators is a single folder copy.</p>
      </article>
    </main>
  </div>
</body>
</html>
