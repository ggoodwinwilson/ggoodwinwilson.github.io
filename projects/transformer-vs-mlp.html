<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Transformer vs. MLP Control Study | Geoff Goodwin-Wilson</title>
  <link rel="stylesheet" href="../styles.css" />
</head>
<body>
  <div class="site">
    <aside class="site-sidebar">
      <div class="avatar" aria-hidden="true">GG</div>
      <div class="brand-name">Geoff Goodwin-Wilson</div>
      <div class="brand-role">Electrical Engineering / Machine Learning</div>
      <p class="sidebar-bio">Quiet logbook for robotics and ML systems built with an electrical engineering toolkit.</p>

      <div class="sidebar-section">
        <div class="sidebar-heading">Downloads</div>
        <div class="sidebar-links">
          <a class="sidebar-link" href="../assets/GeoffGoodwinWilson_Resume.pdf" download>Resume (PDF)</a>
          <a class="sidebar-link" href="../assets/GeoffGoodwinWilson_Portfolio.pdf" download>Portfolio (PDF)</a>
        </div>
      </div>

      <div class="sidebar-section">
        <div class="sidebar-heading">Elsewhere</div>
        <div class="sidebar-links">
          <a class="sidebar-link" href="https://x.com/geoffgw1" target="_blank" rel="noreferrer">
            <svg viewBox="0 0 24 24" aria-hidden="true"><path d="M4 4l7.5 8.3L4.5 20H7l5.3-6.3L16.5 20H20l-7.6-8.4L19.5 4H17l-4.7 5.6L8 4H4z" fill="currentColor" /></svg>
            X / Twitter
          </a>
          <a class="sidebar-link" href="https://github.com/ggoodwinwilson" target="_blank" rel="noreferrer">
            <svg viewBox="0 0 24 24" aria-hidden="true"><path d="M12 2C6.48 2 2 6.58 2 12.17c0 4.48 2.87 8.28 6.84 9.63.5.1.68-.22.68-.48 0-.24-.01-.87-.01-1.7-2.78.62-3.37-1.36-3.37-1.36-.45-1.17-1.1-1.48-1.1-1.48-.9-.63.07-.62.07-.62 1 .07 1.53 1.05 1.53 1.05.89 1.56 2.34 1.11 2.91.85.09-.66.35-1.1.64-1.35-2.22-.26-4.56-1.13-4.56-5 0-1.1.38-2 .99-2.7-.1-.26-.43-1.29.09-2.69 0 0 .83-.27 2.72 1.02a9.2 9.2 0 0 1 4.95 0c1.89-1.29 2.72-1.02 2.72-1.02.52 1.4.19 2.43.09 2.69.62.7 1 1.6 1 2.7 0 3.88-2.34 4.74-4.57 5 .36.32.69.95.69 1.92 0 1.39-.01 2.51-.01 2.85 0 .26.18.58.69.48A10.18 10.18 0 0 0 22 12.17C22 6.58 17.52 2 12 2z" fill="currentColor" /></svg>
            GitHub
          </a>
          <a class="sidebar-link" href="https://www.https://www.linkedin.com/in/geoff-goodwin-wilson-263822126/" target="_blank" rel="noreferrer">
            <svg viewBox="0 0 24 24" aria-hidden="true"><path d="M6.94 21H3.43V8.9h3.51V21zM5.18 7.3c-1.13 0-2.06-.95-2.06-2.12C3.12 4 4.05 3 5.18 3s2.06 1 2.06 2.18c0 1.17-.93 2.12-2.06 2.12zM20.89 21h-3.5v-5.87c0-1.4-.03-3.2-1.94-3.2-1.94 0-2.24 1.5-2.24 3.1V21H9.7V8.9h3.36v1.65h.05c.47-.9 1.63-1.85 3.36-1.85 3.6 0 4.27 2.42 4.27 5.56V21z" fill="currentColor" /></svg>
            LinkedIn
          </a>
        </div>
      </div>
    </aside>

    <main class="site-main">
      <section class="page-intro">
        <a class="back-link" href="../index.html">← Back to projects</a>
        <p class="eyebrow">Project</p>
        <h1 class="page-title">Transformer vs. MLP Control Study</h1>
        <div class="project-meta">2024 · Modeling Study</div>
        <p class="lede">Side-by-side control benchmarks to see when a compact transformer helps over a straightforward MLP policy.</p>
        <div class="meta-grid">
          <div class="meta-box"><strong>Focus</strong><span>Ablations and latency</span></div>
          <div class="meta-box"><strong>Stack</strong><span>Python, PyTorch</span></div>
          <div class="meta-box"><strong>Artifacts</strong><span>Logs, ablation tables, plots</span></div>
        </div>
      </section>

      <article class="entry-content">
        <h2>Overview</h2>
        <p>Kept datasets and training budgets constant to isolate architectural impact. Benchmarks covered smoothness of control, recovery from perturbations, and inference latency.</p>

        <h2>Highlights</h2>
        <ul>
          <li>Attention helped on longer context tasks; MLP held up on short-horizon moves with lower latency.</li>
          <li>Latency and memory cost tracked closely with sequence length; tuned context windows kept the transformer practical.</li>
          <li>Ablation tables capture when to reach for attention versus keep the simple baseline.</li>
        </ul>

        <div class="media-placeholder">Drop comparison plots, confusion matrices, or timing charts here.</div>

        <h2>Notes</h2>
        <p>Inference cost dominates for small devices; trimmed layers and quantization are enough for parity with the MLP on embedded targets.</p>

        <h2>Artifacts</h2>
        <p>Run configs, logs, and plotting notebooks are stored with the project for reruns or extensions.</p>
      </article>
    </main>
  </div>
</body>
</html>
